{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéÆ Workshop de Introdu√ß√£o ao Aprendizado por Refor√ßo\n",
    "‚†Ä\n",
    "\n",
    "Bem vindes ao **Workshop de Introdu√ß√£o ao Aprendizado por Refor√ßo**, organizado pelo Grupo Turing! \n",
    "\n",
    "O objetivo deste evento √© ensinar o b√°sico necess√°rio da √°rea de Aprendizado por Refor√ßo utilizando um dos maiores cl√°ssicos da hist√≥ria dos video-games: ***Pong***.\n",
    "\n",
    "![Pong](https://media2.giphy.com/media/aTGwuEFyg6d8c/giphy.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instala os ambientes do Grupo Turing\n",
    "!pip install -U turing-envs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèì Sobre o Pong\n",
    "\n",
    "Come√ßaremos falando sobre o problema, ou seja, sobre o jogo Pong. Este que foi o primeiro jogo de video-game lucrativo da hist√≥ria, publicado em 1972, constando 48 anos de legado.\n",
    "\n",
    "Pong simula uma partida de t√™nis, existem duas \"raquetes\" e uma bola, e o objetivo de cada uma das raquetes √© n√£o somente evitar que a bola passe por ela, como tamb√©m fazer com que esta passe pela linha que a outra raquete protege, criando assim a premissa que sustenta o interesse pelo jogo. Queremos ent√£o desenvolver um algoritmo capaz de &mdash; sem nenhuma explica√ß√£o adicional &mdash; maximizar as suas recompensas, sendo as a√ß√µes, os estados e as recompensas, todas relativas ao jogo Pong. Teremos no final, portanto, um modelo treinado capaz de bom desempenho dentro do ambiente. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíª Programando..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando o Gym\n",
    "\n",
    "O **[Gym](https://gym.openai.com/)** √© uma biblioteca desenvolvida pela OpenAI que cont√©m v√°rias implementa√ß√µes prontas de ambientes de Aprendizagem por Refor√ßo. Ela √© muito utilizada quando se quer testar um algoritmo de agente sem ter o trabalho de programar seu pr√≥prio ambiente.\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/10624937/42135602-b0335606-7d12-11e8-8689-dd1cf9fa11a9.gif\" alt=\"Exemplos de Ambientes do Gym\" class=\"inline\"/>\n",
    "<figcaption>Exemplo de Ambientes do Gym</figcaption>\n",
    "<br>\n",
    "\n",
    "Para se ter acesso a esses ambientes, basta importar o Gym da seguinte forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O que √© um Ambiente?\n",
    "\n",
    "Um **Ambiente** de Aprendizagem por Refor√ßo √© um espa√ßo que representa o nosso problema, √© o objeto com o qual o nosso agente deve interagir para cumprir sua fun√ß√£o. Isso significa que o agente toma **a√ß√µes** nesse ambiente, e recebe **recompensas** dele com base na qualidade de sua tomada de decis√µes.\n",
    "\n",
    "Todos os ambientes s√£o dotados de um **espa√ßo de observa√ß√µes**, que √© a forma pela qual o agente recebe informa√ß√µes e deve se basear para a tomada de decis√µes, e um **espa√ßo de a√ß√µes**, que especifica as a√ß√µes poss√≠veis do agente. No xadrez, por exemplo, o espa√ßo de observa√ß√µes seria o conjunto de todas as configura√ß√µes diferentes do tabuleiro, e o espa√ßo de a√ß√µes seria o conjunto de todos os movimentos permitidos.\n",
    "\n",
    "<img src=\"https://www.raspberrypi.org/wp-content/uploads/2016/08/giphy-1-1.gif\" alt=\"Uma A√ß√£o do Xadrez\" class=\"inline\"/>\n",
    "\n",
    "### Como Funciona um Ambiente do Gym?\n",
    "\n",
    "Agora que voc√™ j√° sabe o que √© um ambiente, √© preciso entender como nosso agente interage efetivamente com ele. Todos os ambientes do Gym possuem alguns m√©todos simples para facilitar a comunica√ß√£o com eles:\n",
    "\n",
    "<br>\n",
    "\n",
    "| M√©todo         | Funcionalidade                                        |\n",
    "| :------------- |:----------------------------------------------------- |\n",
    "| `reset()`      | Inicializa o ambiente e recebe a observa√ß√£o inicial   |\n",
    "| `step(acao)`   | Executa uma a√ß√£o e recebe a observa√ß√£o e a recompensa |\n",
    "| `render()`     | Renderiza o ambiente                                  |\n",
    "| `close()`      | Fecha o ambiente                                      |\n",
    "\n",
    "<br>\n",
    "\n",
    "Assim, o c√≥digo para interagir com o ambiente costuma seguir o seguinte modelo:\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "env = gym.make(\"Nome do Ambiente\")                   # Cria o ambiente\n",
    "estado = env.reset()                                 # Inicializa o ambiente\n",
    "done = False                                         # Vari√°vel que diz se acabou\n",
    "\n",
    "while not done:\n",
    "    env.render()                                     # Renderiza o ambiente\n",
    "    acao = random()                                  # Define alguma a√ß√£o\n",
    "    estado, recompensa, done, info = env.step(acao)  # Executa uma a√ß√£o\n",
    "    \n",
    "env.close()                                          # Fecha o ambiente\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando um Ambiente\n",
    "\n",
    "Para utilizar um dos ambientes do Gym, n√≥s usamos a fun√ß√£o ```gym.make()```, passando o nome do ambiente desejado como par√¢metro e guardando o valor retornado em uma vari√°vel que chamaramos de ```env```. A lista com todos os ambientes do gym pode ser encontrada [aqui](https://gym.openai.com/envs/#classic_control). Nesse workshop, utilizaremos um ambiente de pong do Grupo Turing, que requer a instala√ß√£o do [Turing Envs](https://github.com/GrupoTuring/turing-envs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/miniconda3/envs/wrkshprl/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"turing_envs:pong-easy-v0\")\n",
    "env.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse caso, n√≥s vamos utilizar o ambiente ```turing_envs:pong-easy-v0```, um ambiente que reproduz o jogo _Pong_.\n",
    "\n",
    "<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAYkAAAEkCAIAAABVPVraAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAEDklEQVR4Xu3csWkDQBBFwZNRCXf9F3hNOFKyYHBg5CeYCX8Bj432sfdeADFfcwAI0CagSJuAIm0CirQJKNImoEibgCJtAoq0CSjSJqBIm4AibQKKtAko0iagSJuAIm0CirQJKNImoEibgCJtAoq0CSjSJqBIm4AibQKKtAko0iagSJuAIm0CirQJKNImoEibgCJtAoqec3i5985prbXWOWdOAH/N3QQUaRNQpE1AkTYBRdoEFGkTUKRNQJE2AUXaBBRpE1CkTUCRNgFF2gQUaRNQpE1AkTYBRdoEFGkTUKRNQJE2AUXaBBRpE1CkTUCRNgFF2gQUaRNQpE1AkTYBRc85EHDvndOvnXPmBB/I3QQUaRNQpE1AkTYBRdoEFGkTUKRNQJE2AUXaBBRpE1CkTUCRNgFF2gQUaRNQpE1AkTYBRdoEFGkTUKRNQJF/4UV+foO7CSjSJqBIm4AibQKKtAko0iagSJuAIm0CirQJKNImoEibgCJtAoq0CSjyhwB4k3vvnNZaPzzecDcBRdoEFGkTUKRNQJE2AUXaBBRpE1CkTUCRNgFF2gQUaRNQpE1AkTYBRdoEFGkTUKRNQJE2AUXaBBRpE1CkTUCRNgFF2gQUaRNQpE1AkTYBRdoEFGkTUKRNQJE2AUXaBBRpE1CkTUCRNgFF2gQUaRNQpE1A0XMOL+ecOQG8i7sJKNImoEibgCJtAoq0CSjSJqBIm4AibQKKtAko0iagSJuAIm0CirQJKNImoEibgCJtAoq0CSjSJqBIm4AibQKKtAko0iagSJuAIm0CirQJKNImoEibgCJtAoq0CSjSJqBIm4AibQKKtAko0iagSJuAIm0CirQJKNImoEibgCJtAoq0CSjSJqBIm4AibQKKtAko0iagSJuAIm0CirQJKNImoEibgCJtAoq0CSjSJqBIm4AibQKKtAko0iagSJuAIm0CirQJKNImoOix954bwH9zNwFF2gQUaRNQpE1AkTYBRdoEFGkTUKRNQJE2AUXaBBRpE1CkTUCRNgFF2gQUaRNQpE1AkTYBRdoEFGkTUKRNQJE2AUXaBBRpE1CkTUCRNgFF2gQUaRNQpE1AkTYBRdoEFGkTUKRNQJE2AUXaBBRpE1CkTUCRNgFF2gQUaRNQpE1AkTYBRdoEFGkTUKRNQJE2AUXaBBRpE1CkTUCRNgFF2gQUaRNQpE1AkTYBRdoEFGkTUKRNQJE2AUXaBBRpE1CkTUCRNgFF2gQUaRNQpE1AkTYBRdoEFGkTUKRNQJE2AUXaBBRpE1CkTUCRNgFF2gQUaRNQpE1AkTYBRdoEFGkTUKRNQJE2AUXaBBRpE1CkTUCRNgFF2gQUaRNQpE1AkTYBRdoEFGkTUKRNQJE2AUXaBBRpE1D0DYCTDEdrWdtnAAAAAElFTkSuQmCC\" width=\"400px\" alt=\"Ambiente do turing_envs:pong-easy-v0\" class=\"inline\"/>\n",
    "\n",
    "#### Caracter√≠sticas do Pong\n",
    "\n",
    "Antes de treinar qualquer agente, primeiro √© preciso entender melhor quais as caracter√≠sticas do nosso ambiente.\n",
    "\n",
    "O **Espa√ßo de Observa√ß√£o** do pong (modo f√°cil) √© definido por 2 informa√ß√µes:\n",
    "\n",
    "| Estado    | Informa√ß√£o                            |\n",
    "| :-------- | :------------------------------------ |\n",
    "| 0         | Dist√¢ncia _x_ entre a bola e o agente |\n",
    "| 1         | Dist√¢ncia _y_ entre a bola e o agente |\n",
    "\n",
    "Dessa forma, a cada instante recebemos uma lista da observa√ß√£o com o seguinte formato:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  28.755857 -291.9781  ]\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J√° o **Espa√ßo de A√ß√£o** √© composto por tr√™s a√ß√µes: mover o jogador para cima, baixo, ou deix√°-lo parado:\n",
    "\n",
    "| A√ß√£o | Significado      |\n",
    "| :--- | :--------------- |\n",
    "| 0    | Ficar parado     |\n",
    "| 1    | Mover para baixo |\n",
    "| 2    | Mover para cima  |\n",
    "\n",
    "Por exemplo, para mover a barra para a cima, fazemos `env.step(2)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim, cada vez que tomamos uma a√ß√£o, recebemos do ambiente uma **recompensa**, conforme a tabela abaixo:\n",
    "\n",
    "| Ocorr√™ncia          | Recompensa |\n",
    "| :------------------ | ---------: |\n",
    "| Ponto do Agente     | $+500$     |\n",
    "| Ponto do Oponente   | $-500$     |\n",
    "| Vit√≥ria do Agente   | $+2000$    |\n",
    "| Vit√≥ria do Oponente | $-2000$    |\n",
    "\n",
    "O primeiro jogador a fazer quatro pontos ganha o jogo. Al√©m disso, as recompensas s√£o cumulativas. Isso significa que se o oponente fizer um ponto _e_ ganhar o jogo, a recompensa √© de $-2500$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úç Exerc√≠cio de Gym\n",
    "\n",
    "Agora que voc√™ j√° entende como o Gym funciona, vamos tentar aplicar esse conhecimento criando uma fun√ß√£o que roda um epis√≥dio de Pong tomando a√ß√µes aleat√≥rias!\n",
    "\n",
    "OBS: Lembrete das fun√ß√µes do Gym\n",
    "\n",
    "| M√©todo                 | Funcionalidade                                          |\n",
    "| :--------------------- |:------------------------------------------------------- |\n",
    "| `reset()`              | Inicializa o ambiente e recebe a observa√ß√£o inicial     |\n",
    "| `step(acao)`           | Executa uma a√ß√£o e recebe a observa√ß√£o e a recompensa   |\n",
    "| `render()`             | Renderiza o ambiente                                    |\n",
    "| `close()`              | Fecha o ambiente                                        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essa fun√ß√£o deve rodar um episodio de Pong escolhendo a√ß√µes aleat√≥rias\n",
    "def rodar_ambiente():\n",
    "    # Criando o ambiente 'turing_envs:pong-easy-v0'\n",
    "    env = gym.make(\"turing_envs:pong-easy-v0\")\n",
    "\n",
    "    # Resete o ambiente e receba o primeiro estado\n",
    "    estado = env.reset()\n",
    "\n",
    "    # Inicializando done como false\n",
    "    done = False\n",
    "\n",
    "    # Loop de treino\n",
    "    while not done:\n",
    "        # Escolha uma acao aleatoria\n",
    "        acao = env.action_space.sample()\n",
    "\n",
    "        # Tome essa acao e receba as informacoes do estado seguinte\n",
    "        prox_estado, recompensa, done, info = env.step(acao)\n",
    "\n",
    "        # Renderize o ambiente\n",
    "        env.render()\n",
    "\n",
    "        # Atualizando o estado\n",
    "        estado = prox_estado\n",
    "\n",
    "    # Fechando o ambiente\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testando a fun√ß√£o\n",
    "rodar_ambiente()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üë©‚Äçüíª Algoritmo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiramente, precisaremos utilizar uma biblioteca chamada ***NumPy*** para auxiliar nas computa√ß√µes. Esta √© uma biblioteca do Python capaz de manusear diversas computa√ß√µes matem√°ticas com maestria e ser√° importante futuramente para o nosso trabalho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N√∫mero de a√ß√µes: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/miniconda3/envs/wrkshprl/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # Importando a biblioteca NumPy\n",
    "import gym         # Importando a Biblioteca Gym\n",
    "\n",
    "# Criando o nosso Ambiente: Pong\n",
    "env = gym.make(\"turing_envs:pong-easy-v0\")\n",
    "\n",
    "# N√∫mero total de a√ß√µes: 3\n",
    "# 0 = parado; 1 = baixo; 2 = cima\n",
    "n_acoes = env.action_space.n\n",
    "\n",
    "print('N√∫mero de a√ß√µes:', n_acoes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî¢ Discretizando o nosso Estado\n",
    "\n",
    "Como comentamos anteriormente, o estado que o nosso agente recebe consiste das dist√¢ncias horizontal e vertical da raquete controlada at√© a bola. Dessa forma, se a nossa tela possuir 800 unidades de largura e 600 unidades de altura, a quantidade total de diferentes estados poss√≠veis seria aproximadamente $3 \\times 800 \\times 600 = 960000$.\n",
    "\n",
    "Como Q-Learning √© um algoritmo que guarda em uma tabela as estimativas do Q de cada a√ß√£o para cada estado, esse gigantesco n√∫mero de estados exigiria n√£o somente guardar como atualizar cada um desses Q. N√£o √© uma situa√ß√£o ideal.\n",
    "\n",
    "Para simplificar (e agilizar) a situa√ß√£o, \"discretizar\" os nossos estados √© razo√°vel e esperado. Faremos com que estados similares o suficiente sejam considerados como iguais e comparilhem das mesmas estimativas, j√° que n√£o faz sentido distinguir o estado (502,234) do estado (515,222)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretiza_estado(estado):\n",
    "    return tuple(round(x/10) for x in estado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÄ Escolhendo A√ß√µes\n",
    "\n",
    "Para o processo de de escolha de a√ß√£o, √© necess√°rio lembrar do dilema entre **Explora√ß√£o** e **Explota√ß√£o**. Nosso modelo precisa estabelecer um equil√≠brio entre **explorar o ambiente**, escolhendo a√ß√µes que ele n√£o costuma tomar para encontrar alguma solu√ß√£o que ele n√£o havia pensado antes, e **aproveitar** o conhecimento que j√° possui, tomando a√ß√µes que ele acredita serem as melhores para maximizar as recompensas que receber√° no epis√≥dio.\n",
    "\n",
    "De forma a assegurar que o agente busque tanto novas alternativas que podem gerar melhores resultados quanto seja capaz de utilizar o aprendizado obtido de forma a maximizar seu retorno, existem diversas estrat√©gias para a escolha de explora√ß√£o e explota√ß√£o. Uma das mais utilizadas, que tamb√©m vamos utilizar aqui, √© a sele√ß√£o de a√ß√µes pela estrat√©gia do **\"$\\epsilon$-greedy\"**.\n",
    "\n",
    "#### A Estrat√©gia **$\\epsilon$-greedy**\n",
    "\n",
    "O algoritmo \"$\\epsilon$-greedy\" √© definido da seguinte forma: √© retirado um n√∫mero aleat√≥rio, no intervalo entre 0 e 1. caso este n√∫mero tenha valor inferior ao valor do epsilon, a escolha ser√° de uma a√ß√£o aleat√≥ria, o que configura explora√ß√£o. Caso este n√∫mero seja superior ao epsilon, a a√ß√£o a ser tomada √© a que gera a maior recompensa de acordo com os valores da tabela Q.\n",
    "\n",
    "Este valor de $\\epsilon$ n√£o √© constante ao longo do treinamento. Inicialmente, este valor √© alto, incentivando a maior explora√ß√£o do ambiente. A medida que o treinamento ocorre, mais informa√ß√£o sobre o ambiente √© adquirida, conseguindo uma tabela Q mais representativa da realidade. Dessa forma, quanto mais avan√ßado no treinamento, menor a necessidade de explora√ß√£o e maior a necessidade de exploitar o conhecimento adquirido para maximizar a recompensa. Esta atualiza√ß√£o do $\\epsilon$ √© chamada **\"$\\epsilon$-decay\"** (decaimento do epsilon). Tamb√©m √© estabelecido um valor m√≠nimo para o $\\epsilon$, para que o agente nunca pare completamente de explorar o ambiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constantes da Pol√≠tica Epsilon Greedy\n",
    "# Epsilon: probabilidade de experimentar uma a√ß√£o aleat√≥ria\n",
    "EPSILON = 0.7        # Valor inicial do epsilon\n",
    "EPSILON_MIN = 0.01   # Valor m√≠nimo de epsilon\n",
    "DECAIMENTO = 0.98    # Fator de deca√≠mento do epsilon (por epis√≥dio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def escolhe_acao(env, Q, estado, epsilon):\n",
    "    # Se n√£o conhecermos ainda o estado, inicializamos o Q de cada a√ß√£o como 0\n",
    "    if estado not in Q.keys(): Q[estado] = [0] * n_acoes\n",
    "\n",
    "    # Escolhemos um n√∫mero aleat√≥rio com \"np.random.random()\"\n",
    "    # Se esse n√∫mero for menor que epsilon, tomamos uma a√ß√£o aleat√≥ria\n",
    "    if np.random.random() < epsilon:\n",
    "        # Escolhemos uma a√ß√£o aleat√≥ria, com env.action_space.sample()\n",
    "        acao = env.action_space.sample()\n",
    "    else:\n",
    "        # Escolhemos a melhor a√ß√£o para o estado atual, com np.argmax()\n",
    "        acao = np.argmax(Q[estado])\n",
    "    return acao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para rodar uma partida, s√£o necess√°rias algumas etapas. Inicialmente, o ambiente √© reiniciado, de forma a inicar um novo epis√≥dio. Em seguida, √© necess√°rio discretizar o estado, pelos motivos j√° explicados acima. Esta discretiza√ß√£o deve ocorrer toda vez em que estamos em um novo estado.\n",
    "\n",
    "Enquanto o ambiente n√£o chega em seu estado terminal, indicado pela vari√°vel \"done\", ser√° feito o processo de escolha de a√ß√µes e, uma vez escolhida, deve-se receber do ambiente o pr√≥ximo estado, a recompensa que a a√ß√£o escolhida gerou, al√©m do sinal se estamos no estado terminal. Todo o processo √© repetido novamente para o pr√≥ximo estado, at√© o final do epis√≥dio.\n",
    "\n",
    "Como explicado na se√ß√£o sobre a biblioteca \"Gym\", \"env.render()\" tem como papel mostrar o ambiente (neste caso, a partida de Pong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roda_partida(env, Q, renderiza=True):\n",
    "    # Resetamos o ambiente\n",
    "    estado = env.reset()\n",
    "\n",
    "    # Discretizamos o estado\n",
    "    estado = discretiza_estado(estado)\n",
    "    \n",
    "    done = False\n",
    "    retorno = 0\n",
    "    \n",
    "    while not done:\n",
    "        # Escolhemos uma a√ß√£o\n",
    "        acao = escolhe_acao(env, Q, estado, 0.0)\n",
    "\n",
    "        # Tomamos nossa a√ß√£o escolhida e recebemos informa√ß√µes do pr√≥ximo estado\n",
    "        prox_estado, recompensa, done, info = env.step(acao)\n",
    "\n",
    "        # Discretizamos o pr√≥ximo estado\n",
    "        prox_estado = discretiza_estado(prox_estado)\n",
    "\n",
    "        # Renderizamos o ambiente\n",
    "        if renderiza:\n",
    "            env.render()\n",
    "\n",
    "        retorno += recompensa\n",
    "        estado = prox_estado\n",
    "\n",
    "    print(f'retorno {retorno:.1f},  '\n",
    "          f'placar {env.score[0]}x{env.score[1]}')\n",
    "    \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retorno -3500.0,  placar 1x4\n"
     ]
    }
   ],
   "source": [
    "# Rodamos uma partida de Pong\n",
    "Q = {}\n",
    "roda_partida(env, Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèãÔ∏è‚Äç‚ôÄÔ∏è Treinamento\n",
    "\n",
    "Agora sim chegaremos no treinamento propriamente dito. Usando os conceitos vistos na apresenta√ß√£o e nas se√ß√µes anteriores do notebook, podemos definir a fun√ß√£o de treinamento que vai permitir que o agente aprenda a jogar PONG por meio de Q-Learning tabular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O algoritmo se baseia na atualiza√ß√£o de estimativas dos valores Q para cada par estado-a√ß√£o, de forma a chegar a uma tabela cada vez mais pr√≥xima da realidade do ambiente. Dessa forma, devemos atualizar cada entrada da tabela de acordo com a **equa√ß√£o do Q-Learning**:\n",
    "\n",
    "$$Q*(s,a) \\leftarrow Q*(s,a) + \\alpha \\cdot \\left[r + \\gamma \\cdot \\max_{a'} (Q(s',a')) - Q(s, a)\\right]$$\n",
    "\n",
    "Esta equa√ß√£o corrige o valor do Q(s,a) de acordo com os valores anteriores somados a uma parecela de corre√ß√£o, de forma a minimizar o erro. A recompensa √© representada por r, enquanto os outros par√¢metros est√£o explicados a seguir:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \"ALFA\" ($\\alpha$): algoritmos de aprendizado de m√°quina costumam precisar de uma forma de serem otimizados. Q-learning trabalha em cima de gradientes, uma entidade matem√°tica que indica a dire√ß√£o para maximizar (ou minimizar) uma fun√ß√£o. Dispondo dessa dire√ß√£o, precisamos informar qual deve ser o tamanho do passo a ser dado antes de atualizar a nova \"dire√ß√£o ideal\".\n",
    "\n",
    "* \"GAMA\" ($\\gamma$): denota o quanto desejamos que nosso algoritmo considere eventos futuros. Se \"$\\gamma = 1$\", nosso algoritmo avaliar√° que a situa√ß√£o futura ser melhor que a atual √© t√£o importante quanto a recompensa da situa√ß√£o atual em si, por outro lado, se \"$\\gamma = 0$\", os eventos futuros n√£o apresentam import√¢ncia alguma para nosso algoritmo. \n",
    "\n",
    "* \"Q\" √© um dicion√°rio, ou seja, uma estrtura de dados capaz de buscar elementos de forma r√°pida. N√≥s o usaremos para guardar valores relativos √†s estimativas do algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperpar√¢metros do Q-Learning\n",
    "ALFA = 0.05          # Learning rate\n",
    "GAMA = 0.9           # Fator de desconto\n",
    "\n",
    "# Dicion√°rio dos valores de Q\n",
    "# Chaves: estados; valores: qualidade Q atribuida a cada a√ß√£o\n",
    "Q = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atualiza_q(Q, estado, acao, recompensa, prox_estado):\n",
    "    # para cada estado ainda n√£o descoberto, iniciamos seu valor como nulo\n",
    "    if estado not in Q.keys(): Q[estado] = [0] * n_acoes\n",
    "    if prox_estado not in Q.keys(): Q[prox_estado] = [0] * n_acoes\n",
    "\n",
    "    # equa√ß√£o do Q-Learning\n",
    "    Q[estado][acao] += ALFA * (recompensa + GAMA*np.max(Q[prox_estado]) - Q[estado][acao])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle √© uma maneira de salvar dados em um arquivo independente. Dessa forma, podemos gravar os valores da nossa tabela Q em um arquivo pr√≥prio, ficando dispon√≠vel para ser acessada em outro momento. Assim, podemos efetivamente salvar o modelo treinado para ser utilizado posteriormente. Abaixo, j√° est√£o presentes as fun√ß√µes de salvar e de abrir as tabelas com pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def salva_tabela(Q, nome = 'model.pickle'):\n",
    "    with open(nome, 'wb') as pickle_out:\n",
    "        pickle.dump(Q, pickle_out)\n",
    "\n",
    "def carrega_tabela(nome = 'model.pickle'):\n",
    "    with open(nome, 'rb') as pickle_out:\n",
    "        return pickle.load(pickle_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fun√ß√£o de treinamento tem estrutura semelhante √† fun√ß√£o roda_partida, conforme visto anteriormente. A cada epis√≥dio, o embiente deve ser reiniciado e discretizado, e deve indicar que o epis√≥dio ainda n√£o chegou em sua condi√ß√£o terminal. Devemos tamb√©m zerar o valor da recompensa, pois n√£o devemos utilizar o retorno do epis√≥dio anterior.\n",
    "\n",
    "Enquanto o epis√≥dio n√£o chega no final, o agente deve escolher uma a√ß√£o e tomar a a√ß√£o escolhida. Uma vez tomada a a√ß√£o, o ambiente fornece o pr√≥ximo estado, a recompensa recebida com a escolha, a indica√ß√£o se o estado √© terminal e informa√ß√µes sobre o ambiente.\n",
    "\n",
    "Em seguida, devemos discretizar o pr√≥ximo estado e atualizar os valores de q, o retorno e o estado atual.\n",
    "\n",
    "Por fim, devemos atualizar o valor do epsilon, de acordo com o m√©todo $\\epsilon$-greedy, onde deve ocorrer o decaimento do epsilon, mas seu valor nunca deve ser inferior ao valor m√≠nimo definido.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `N_EPISODIOS` dita quantas vezes o agente dever√° \"reviver\" o ambiente (vit√≥rias e derrotas) antes de acabar seu treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPISODIOS = 250    # quantidade de epis√≥dios que treinaremos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treina(env, Q):\n",
    "    retornos = []      # retorno de cada epis√≥dio\n",
    "    epsilon = EPSILON\n",
    "\n",
    "    for episodio in range(1, N_EPISODIOS+1):\n",
    "        # resetar o ambiente\n",
    "        estado = env.reset()\n",
    "\n",
    "        # discretizar o estado inicial\n",
    "        estado = discretiza_estado(estado)\n",
    "        \n",
    "        done = False\n",
    "        retorno = 0\n",
    "        \n",
    "        while not done:\n",
    "            # escolher uma a√ß√£o\n",
    "            acao = escolhe_acao(env, Q, estado, epsilon)\n",
    "\n",
    "            # tomar a a√ß√£o\n",
    "            prox_estado, recompensa, done, info = env.step(acao)\n",
    "\n",
    "            # discretizar o pr√≥ximo estado\n",
    "            prox_estado = discretiza_estado(prox_estado)\n",
    "\n",
    "            atualiza_q(Q, estado, acao, recompensa, prox_estado)\n",
    "\n",
    "            retorno += recompensa\n",
    "            estado = prox_estado\n",
    "\n",
    "        # calcular o pr√≥ximo epsilon\n",
    "        epsilon *= DECAIMENTO\n",
    "        epsilon = max(epsilon, EPSILON_MIN)\n",
    "\n",
    "        retornos.append(retorno)\n",
    "\n",
    "        if episodio % 10 == 0:\n",
    "            salva_tabela(Q)\n",
    "\n",
    "        print(f'epis√≥dio {episodio},  '\n",
    "              f'retorno {retorno:7.1f},  '\n",
    "              f'retorno m√©dio (√∫ltimos 10 epis√≥dios) {np.mean(retornos[-10:]):7.1f},  '\n",
    "              f'placar {env.score[0]}x{env.score[1]},  '\n",
    "              f'epsilon: {epsilon:.3f}')\n",
    "        \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis√≥dio 1,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -4000.0,  placar 0x4,  epsilon: 0.686\n",
      "epis√≥dio 2,  retorno -3000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3500.0,  placar 2x4,  epsilon: 0.672\n",
      "epis√≥dio 3,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3666.7,  placar 0x4,  epsilon: 0.659\n",
      "epis√≥dio 4,  retorno -3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3625.0,  placar 1x4,  epsilon: 0.646\n",
      "epis√≥dio 5,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3700.0,  placar 0x4,  epsilon: 0.633\n",
      "epis√≥dio 6,  retorno -3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3666.7,  placar 1x4,  epsilon: 0.620\n",
      "epis√≥dio 7,  retorno -3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3642.9,  placar 1x4,  epsilon: 0.608\n",
      "epis√≥dio 8,  retorno -2500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3500.0,  placar 3x4,  epsilon: 0.596\n",
      "epis√≥dio 9,  retorno -3000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3444.4,  placar 2x4,  epsilon: 0.584\n",
      "epis√≥dio 10,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3500.0,  placar 0x4,  epsilon: 0.572\n",
      "epis√≥dio 11,  retorno -3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3450.0,  placar 1x4,  epsilon: 0.561\n",
      "epis√≥dio 12,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3550.0,  placar 0x4,  epsilon: 0.549\n",
      "epis√≥dio 13,  retorno -3000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3450.0,  placar 2x4,  epsilon: 0.538\n",
      "epis√≥dio 14,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3500.0,  placar 0x4,  epsilon: 0.528\n",
      "epis√≥dio 15,  retorno -3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3450.0,  placar 1x4,  epsilon: 0.517\n",
      "epis√≥dio 16,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3500.0,  placar 0x4,  epsilon: 0.507\n",
      "epis√≥dio 17,  retorno -3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3500.0,  placar 1x4,  epsilon: 0.497\n",
      "epis√≥dio 18,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3650.0,  placar 0x4,  epsilon: 0.487\n",
      "epis√≥dio 19,  retorno -2500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3600.0,  placar 3x4,  epsilon: 0.477\n",
      "epis√≥dio 20,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3600.0,  placar 0x4,  epsilon: 0.467\n",
      "epis√≥dio 21,  retorno -3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3600.0,  placar 1x4,  epsilon: 0.458\n",
      "epis√≥dio 22,  retorno -3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3550.0,  placar 1x4,  epsilon: 0.449\n",
      "epis√≥dio 23,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3650.0,  placar 0x4,  epsilon: 0.440\n",
      "epis√≥dio 24,  retorno -3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3600.0,  placar 1x4,  epsilon: 0.431\n",
      "epis√≥dio 25,  retorno -3000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3550.0,  placar 2x4,  epsilon: 0.422\n",
      "epis√≥dio 26,  retorno -3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3500.0,  placar 1x4,  epsilon: 0.414\n",
      "epis√≥dio 27,  retorno -3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3500.0,  placar 1x4,  epsilon: 0.406\n",
      "epis√≥dio 28,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3500.0,  placar 0x4,  epsilon: 0.398\n",
      "epis√≥dio 29,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3650.0,  placar 0x4,  epsilon: 0.390\n",
      "epis√≥dio 30,  retorno -3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3600.0,  placar 1x4,  epsilon: 0.382\n",
      "epis√≥dio 31,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3650.0,  placar 0x4,  epsilon: 0.374\n",
      "epis√≥dio 32,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3700.0,  placar 0x4,  epsilon: 0.367\n",
      "epis√≥dio 33,  retorno -3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3650.0,  placar 1x4,  epsilon: 0.359\n",
      "epis√≥dio 34,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3700.0,  placar 0x4,  epsilon: 0.352\n",
      "epis√≥dio 35,  retorno -3000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3700.0,  placar 2x4,  epsilon: 0.345\n",
      "epis√≥dio 36,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3750.0,  placar 0x4,  epsilon: 0.338\n",
      "epis√≥dio 37,  retorno -3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3750.0,  placar 1x4,  epsilon: 0.331\n",
      "epis√≥dio 38,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3750.0,  placar 0x4,  epsilon: 0.325\n",
      "epis√≥dio 39,  retorno -3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3700.0,  placar 1x4,  epsilon: 0.318\n",
      "epis√≥dio 40,  retorno -3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3700.0,  placar 1x4,  epsilon: 0.312\n",
      "epis√≥dio 41,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3700.0,  placar 0x4,  epsilon: 0.306\n",
      "epis√≥dio 42,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3700.0,  placar 0x4,  epsilon: 0.300\n",
      "epis√≥dio 43,  retorno -3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3700.0,  placar 1x4,  epsilon: 0.294\n",
      "epis√≥dio 44,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3700.0,  placar 0x4,  epsilon: 0.288\n",
      "epis√≥dio 45,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3800.0,  placar 0x4,  epsilon: 0.282\n",
      "epis√≥dio 46,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3800.0,  placar 0x4,  epsilon: 0.276\n",
      "epis√≥dio 47,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3850.0,  placar 0x4,  epsilon: 0.271\n",
      "epis√≥dio 48,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3850.0,  placar 0x4,  epsilon: 0.265\n",
      "epis√≥dio 49,  retorno -3000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3800.0,  placar 2x4,  epsilon: 0.260\n",
      "epis√≥dio 50,  retorno -3000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3750.0,  placar 2x4,  epsilon: 0.255\n",
      "epis√≥dio 51,  retorno -3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3700.0,  placar 1x4,  epsilon: 0.250\n",
      "epis√≥dio 52,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3700.0,  placar 0x4,  epsilon: 0.245\n",
      "epis√≥dio 53,  retorno -3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3700.0,  placar 1x4,  epsilon: 0.240\n",
      "epis√≥dio 54,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3700.0,  placar 0x4,  epsilon: 0.235\n",
      "epis√≥dio 55,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3700.0,  placar 0x4,  epsilon: 0.230\n",
      "epis√≥dio 56,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3700.0,  placar 0x4,  epsilon: 0.226\n",
      "epis√≥dio 57,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3700.0,  placar 0x4,  epsilon: 0.221\n",
      "epis√≥dio 58,  retorno -3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3650.0,  placar 1x4,  epsilon: 0.217\n",
      "epis√≥dio 59,  retorno -3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3700.0,  placar 1x4,  epsilon: 0.213\n",
      "epis√≥dio 60,  retorno -2500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3650.0,  placar 3x4,  epsilon: 0.208\n",
      "epis√≥dio 61,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3700.0,  placar 0x4,  epsilon: 0.204\n",
      "epis√≥dio 62,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3700.0,  placar 0x4,  epsilon: 0.200\n",
      "epis√≥dio 63,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3750.0,  placar 0x4,  epsilon: 0.196\n",
      "epis√≥dio 64,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3750.0,  placar 0x4,  epsilon: 0.192\n",
      "epis√≥dio 65,  retorno -2500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3600.0,  placar 3x4,  epsilon: 0.188\n",
      "epis√≥dio 66,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3600.0,  placar 0x4,  epsilon: 0.185\n",
      "epis√≥dio 67,  retorno -3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3550.0,  placar 1x4,  epsilon: 0.181\n",
      "epis√≥dio 68,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3600.0,  placar 0x4,  epsilon: 0.177\n",
      "epis√≥dio 69,  retorno -3000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3550.0,  placar 2x4,  epsilon: 0.174\n",
      "epis√≥dio 70,  retorno -3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3650.0,  placar 1x4,  epsilon: 0.170\n",
      "epis√≥dio 71,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3650.0,  placar 0x4,  epsilon: 0.167\n",
      "epis√≥dio 72,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3650.0,  placar 0x4,  epsilon: 0.163\n",
      "epis√≥dio 73,  retorno -3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3600.0,  placar 1x4,  epsilon: 0.160\n",
      "epis√≥dio 74,  retorno -3000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3500.0,  placar 2x4,  epsilon: 0.157\n",
      "epis√≥dio 75,  retorno -3000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3550.0,  placar 2x4,  epsilon: 0.154\n",
      "epis√≥dio 76,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3550.0,  placar 0x4,  epsilon: 0.151\n",
      "epis√≥dio 77,  retorno -3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3550.0,  placar 1x4,  epsilon: 0.148\n",
      "epis√≥dio 78,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3550.0,  placar 0x4,  epsilon: 0.145\n",
      "epis√≥dio 79,  retorno -3000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3550.0,  placar 2x4,  epsilon: 0.142\n",
      "epis√≥dio 80,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3600.0,  placar 0x4,  epsilon: 0.139\n",
      "epis√≥dio 81,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3600.0,  placar 0x4,  epsilon: 0.136\n",
      "epis√≥dio 82,  retorno -3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3550.0,  placar 1x4,  epsilon: 0.134\n",
      "epis√≥dio 83,  retorno -3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3550.0,  placar 1x4,  epsilon: 0.131\n",
      "epis√≥dio 84,  retorno -2500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3500.0,  placar 3x4,  epsilon: 0.128\n",
      "epis√≥dio 85,  retorno -3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3550.0,  placar 1x4,  epsilon: 0.126\n",
      "epis√≥dio 86,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3550.0,  placar 0x4,  epsilon: 0.123\n",
      "epis√≥dio 87,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3600.0,  placar 0x4,  epsilon: 0.121\n",
      "epis√≥dio 88,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3600.0,  placar 0x4,  epsilon: 0.118\n",
      "epis√≥dio 89,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3700.0,  placar 0x4,  epsilon: 0.116\n",
      "epis√≥dio 90,  retorno -3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3650.0,  placar 1x4,  epsilon: 0.114\n",
      "epis√≥dio 91,  retorno -3000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3550.0,  placar 2x4,  epsilon: 0.111\n",
      "epis√≥dio 92,  retorno -3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3550.0,  placar 1x4,  epsilon: 0.109\n",
      "epis√≥dio 93,  retorno -3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3550.0,  placar 1x4,  epsilon: 0.107\n",
      "epis√≥dio 94,  retorno -2500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3550.0,  placar 3x4,  epsilon: 0.105\n",
      "epis√≥dio 95,  retorno -2500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3450.0,  placar 3x4,  epsilon: 0.103\n",
      "epis√≥dio 96,  retorno -3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3400.0,  placar 1x4,  epsilon: 0.101\n",
      "epis√≥dio 97,  retorno -3000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3300.0,  placar 2x4,  epsilon: 0.099\n",
      "epis√≥dio 98,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3300.0,  placar 0x4,  epsilon: 0.097\n",
      "epis√≥dio 99,  retorno -3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3250.0,  placar 1x4,  epsilon: 0.095\n",
      "epis√≥dio 100,  retorno  4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -2500.0,  placar 4x0,  epsilon: 0.093\n",
      "epis√≥dio 101,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -2600.0,  placar 0x4,  epsilon: 0.091\n",
      "epis√≥dio 102,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -2650.0,  placar 0x4,  epsilon: 0.089\n",
      "epis√≥dio 103,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -2700.0,  placar 0x4,  epsilon: 0.087\n",
      "epis√≥dio 104,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -2850.0,  placar 0x4,  epsilon: 0.086\n",
      "epis√≥dio 105,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3000.0,  placar 0x4,  epsilon: 0.084\n",
      "epis√≥dio 106,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3050.0,  placar 0x4,  epsilon: 0.082\n",
      "epis√≥dio 107,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3150.0,  placar 0x4,  epsilon: 0.081\n",
      "epis√≥dio 108,  retorno -3000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3050.0,  placar 2x4,  epsilon: 0.079\n",
      "epis√≥dio 109,  retorno -3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3050.0,  placar 1x4,  epsilon: 0.077\n",
      "epis√≥dio 110,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3850.0,  placar 0x4,  epsilon: 0.076\n",
      "epis√≥dio 111,  retorno -2500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3700.0,  placar 3x4,  epsilon: 0.074\n",
      "epis√≥dio 112,  retorno -3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3650.0,  placar 1x4,  epsilon: 0.073\n",
      "epis√≥dio 113,  retorno -3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3600.0,  placar 1x4,  epsilon: 0.071\n",
      "epis√≥dio 114,  retorno -3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3550.0,  placar 1x4,  epsilon: 0.070\n",
      "epis√≥dio 115,  retorno -2500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3400.0,  placar 3x4,  epsilon: 0.069\n",
      "epis√≥dio 116,  retorno -3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -3350.0,  placar 1x4,  epsilon: 0.067\n",
      "epis√≥dio 117,  retorno  2500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -2700.0,  placar 4x3,  epsilon: 0.066\n",
      "epis√≥dio 118,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -2800.0,  placar 0x4,  epsilon: 0.065\n",
      "epis√≥dio 119,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -2850.0,  placar 0x4,  epsilon: 0.063\n",
      "epis√≥dio 120,  retorno -3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -2800.0,  placar 1x4,  epsilon: 0.062\n",
      "epis√≥dio 121,  retorno -3000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -2850.0,  placar 2x4,  epsilon: 0.061\n",
      "epis√≥dio 122,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -2900.0,  placar 0x4,  epsilon: 0.060\n",
      "epis√≥dio 123,  retorno -3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -2900.0,  placar 1x4,  epsilon: 0.058\n",
      "epis√≥dio 124,  retorno  3000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -2250.0,  placar 4x2,  epsilon: 0.057\n",
      "epis√≥dio 125,  retorno -3000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -2300.0,  placar 2x4,  epsilon: 0.056\n",
      "epis√≥dio 126,  retorno  3000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -1650.0,  placar 4x2,  epsilon: 0.055\n",
      "epis√≥dio 127,  retorno -4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -2300.0,  placar 0x4,  epsilon: 0.054\n",
      "epis√≥dio 128,  retorno  3000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -1600.0,  placar 4x2,  epsilon: 0.053\n",
      "epis√≥dio 129,  retorno -3000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios) -1500.0,  placar 2x4,  epsilon: 0.052\n",
      "epis√≥dio 130,  retorno  3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  -800.0,  placar 4x1,  epsilon: 0.051\n",
      "epis√≥dio 131,  retorno -2500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  -750.0,  placar 3x4,  epsilon: 0.050\n",
      "epis√≥dio 132,  retorno -2500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  -600.0,  placar 3x4,  epsilon: 0.049\n",
      "epis√≥dio 133,  retorno  3000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)    50.0,  placar 4x2,  epsilon: 0.048\n",
      "epis√≥dio 134,  retorno  2500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)     0.0,  placar 4x3,  epsilon: 0.047\n",
      "epis√≥dio 135,  retorno -3000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)     0.0,  placar 2x4,  epsilon: 0.046\n",
      "epis√≥dio 136,  retorno  3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)    50.0,  placar 4x1,  epsilon: 0.045\n",
      "epis√≥dio 137,  retorno  3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)   800.0,  placar 4x1,  epsilon: 0.044\n",
      "epis√≥dio 138,  retorno  3000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)   800.0,  placar 4x2,  epsilon: 0.043\n",
      "epis√≥dio 139,  retorno  3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  1450.0,  placar 4x1,  epsilon: 0.042\n",
      "epis√≥dio 140,  retorno -3000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)   800.0,  placar 2x4,  epsilon: 0.041\n",
      "epis√≥dio 141,  retorno  3000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  1350.0,  placar 4x2,  epsilon: 0.041\n",
      "epis√≥dio 142,  retorno  4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  2000.0,  placar 4x0,  epsilon: 0.040\n",
      "epis√≥dio 143,  retorno  4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  2100.0,  placar 4x0,  epsilon: 0.039\n",
      "epis√≥dio 144,  retorno  4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  2250.0,  placar 4x0,  epsilon: 0.038\n",
      "epis√≥dio 145,  retorno  3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  2900.0,  placar 4x1,  epsilon: 0.037\n",
      "epis√≥dio 146,  retorno  3000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  2850.0,  placar 4x2,  epsilon: 0.037\n",
      "epis√≥dio 147,  retorno  4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  2900.0,  placar 4x0,  epsilon: 0.036\n",
      "epis√≥dio 148,  retorno -3000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  2300.0,  placar 2x4,  epsilon: 0.035\n",
      "epis√≥dio 149,  retorno  3000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  2250.0,  placar 4x2,  epsilon: 0.034\n",
      "epis√≥dio 150,  retorno  3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  2900.0,  placar 4x1,  epsilon: 0.034\n",
      "epis√≥dio 151,  retorno  3000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  2900.0,  placar 4x2,  epsilon: 0.033\n",
      "epis√≥dio 152,  retorno  3000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  2800.0,  placar 4x2,  epsilon: 0.032\n",
      "epis√≥dio 153,  retorno  4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  2800.0,  placar 4x0,  epsilon: 0.032\n",
      "epis√≥dio 154,  retorno  2500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  2650.0,  placar 4x3,  epsilon: 0.031\n",
      "epis√≥dio 155,  retorno  3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  2650.0,  placar 4x1,  epsilon: 0.031\n",
      "epis√≥dio 156,  retorno  3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  2700.0,  placar 4x1,  epsilon: 0.030\n",
      "epis√≥dio 157,  retorno  3000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  2600.0,  placar 4x2,  epsilon: 0.029\n",
      "epis√≥dio 158,  retorno -3000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  2600.0,  placar 2x4,  epsilon: 0.029\n",
      "epis√≥dio 159,  retorno  3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  2650.0,  placar 4x1,  epsilon: 0.028\n",
      "epis√≥dio 160,  retorno -1000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  2200.0,  placar 1x3,  epsilon: 0.028\n",
      "epis√≥dio 161,  retorno -3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  1550.0,  placar 1x4,  epsilon: 0.027\n",
      "epis√≥dio 162,  retorno  3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  1600.0,  placar 4x1,  epsilon: 0.027\n",
      "epis√≥dio 163,  retorno  3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  1550.0,  placar 4x1,  epsilon: 0.026\n",
      "epis√≥dio 164,  retorno  3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  1650.0,  placar 4x1,  epsilon: 0.025\n",
      "epis√≥dio 165,  retorno  2500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  1550.0,  placar 4x3,  epsilon: 0.025\n",
      "epis√≥dio 166,  retorno  3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  1550.0,  placar 4x1,  epsilon: 0.024\n",
      "epis√≥dio 167,  retorno -2500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  1000.0,  placar 3x4,  epsilon: 0.024\n",
      "epis√≥dio 168,  retorno  3000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  1600.0,  placar 4x2,  epsilon: 0.024\n",
      "epis√≥dio 169,  retorno  3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  1600.0,  placar 4x1,  epsilon: 0.023\n",
      "epis√≥dio 170,  retorno  3000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  2000.0,  placar 4x2,  epsilon: 0.023\n",
      "epis√≥dio 171,  retorno  4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  2750.0,  placar 4x0,  epsilon: 0.022\n",
      "epis√≥dio 172,  retorno  3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  2750.0,  placar 4x1,  epsilon: 0.022\n",
      "epis√≥dio 173,  retorno  3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  2750.0,  placar 4x1,  epsilon: 0.021\n",
      "epis√≥dio 174,  retorno   500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  2450.0,  placar 1x0,  epsilon: 0.021\n",
      "epis√≥dio 175,  retorno  3000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  2500.0,  placar 4x2,  epsilon: 0.020\n",
      "epis√≥dio 176,  retorno -2500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  1900.0,  placar 3x4,  epsilon: 0.020\n",
      "epis√≥dio 177,  retorno  3000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  2450.0,  placar 4x2,  epsilon: 0.020\n",
      "epis√≥dio 178,  retorno  3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  2500.0,  placar 4x1,  epsilon: 0.019\n",
      "epis√≥dio 179,  retorno  4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  2550.0,  placar 4x0,  epsilon: 0.019\n",
      "epis√≥dio 180,  retorno  4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  2650.0,  placar 4x0,  epsilon: 0.018\n",
      "epis√≥dio 181,  retorno  4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  2650.0,  placar 4x0,  epsilon: 0.018\n",
      "epis√≥dio 182,  retorno  3000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  2600.0,  placar 4x2,  epsilon: 0.018\n",
      "epis√≥dio 183,  retorno  3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  2600.0,  placar 4x1,  epsilon: 0.017\n",
      "epis√≥dio 184,  retorno  2500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  2800.0,  placar 4x3,  epsilon: 0.017\n",
      "epis√≥dio 185,  retorno  4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  2900.0,  placar 4x0,  epsilon: 0.017\n",
      "epis√≥dio 186,  retorno  4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  3550.0,  placar 4x0,  epsilon: 0.016\n",
      "epis√≥dio 187,  retorno  3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  3600.0,  placar 4x1,  epsilon: 0.016\n",
      "epis√≥dio 188,  retorno     0.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  3250.0,  placar 0x0,  epsilon: 0.016\n",
      "epis√≥dio 189,  retorno  3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  3200.0,  placar 4x1,  epsilon: 0.015\n",
      "epis√≥dio 190,  retorno  3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  3150.0,  placar 4x1,  epsilon: 0.015\n",
      "epis√≥dio 191,  retorno  4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  3150.0,  placar 4x0,  epsilon: 0.015\n",
      "epis√≥dio 192,  retorno  3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  3200.0,  placar 4x1,  epsilon: 0.014\n",
      "epis√≥dio 193,  retorno  4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  3250.0,  placar 4x0,  epsilon: 0.014\n",
      "epis√≥dio 194,  retorno -1000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  2900.0,  placar 0x2,  epsilon: 0.014\n",
      "epis√≥dio 195,  retorno  4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  2900.0,  placar 4x0,  epsilon: 0.014\n",
      "epis√≥dio 196,  retorno  3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  2850.0,  placar 4x1,  epsilon: 0.013\n",
      "epis√≥dio 197,  retorno     0.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  2500.0,  placar 1x1,  epsilon: 0.013\n",
      "epis√≥dio 198,  retorno  3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  2850.0,  placar 4x1,  epsilon: 0.013\n",
      "epis√≥dio 199,  retorno  4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  2900.0,  placar 4x0,  epsilon: 0.013\n",
      "epis√≥dio 200,  retorno  3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  2900.0,  placar 4x1,  epsilon: 0.012\n",
      "epis√≥dio 201,  retorno  3000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  2800.0,  placar 4x2,  epsilon: 0.012\n",
      "epis√≥dio 202,  retorno  4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  2850.0,  placar 4x0,  epsilon: 0.012\n",
      "epis√≥dio 203,  retorno  4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  2850.0,  placar 4x0,  epsilon: 0.012\n",
      "epis√≥dio 204,  retorno  4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  3350.0,  placar 4x0,  epsilon: 0.011\n",
      "epis√≥dio 205,  retorno  4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  3350.0,  placar 4x0,  epsilon: 0.011\n",
      "epis√≥dio 206,  retorno  3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  3350.0,  placar 4x1,  epsilon: 0.011\n",
      "epis√≥dio 207,  retorno  4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  3750.0,  placar 4x0,  epsilon: 0.011\n",
      "epis√≥dio 208,  retorno  4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  3800.0,  placar 4x0,  epsilon: 0.010\n",
      "epis√≥dio 209,  retorno  4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  3800.0,  placar 4x0,  epsilon: 0.010\n",
      "epis√≥dio 210,  retorno  3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  3800.0,  placar 4x1,  epsilon: 0.010\n",
      "epis√≥dio 211,  retorno  4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  3900.0,  placar 4x0,  epsilon: 0.010\n",
      "epis√≥dio 212,  retorno  4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  3900.0,  placar 4x0,  epsilon: 0.010\n",
      "epis√≥dio 213,  retorno  4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  3900.0,  placar 4x0,  epsilon: 0.010\n",
      "epis√≥dio 214,  retorno  4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  3900.0,  placar 4x0,  epsilon: 0.010\n",
      "epis√≥dio 215,  retorno  4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  3900.0,  placar 4x0,  epsilon: 0.010\n",
      "epis√≥dio 216,  retorno  3000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  3850.0,  placar 4x2,  epsilon: 0.010\n",
      "epis√≥dio 217,  retorno     0.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  3450.0,  placar 0x0,  epsilon: 0.010\n",
      "epis√≥dio 218,  retorno  3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  3400.0,  placar 4x1,  epsilon: 0.010\n",
      "epis√≥dio 219,  retorno  4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  3400.0,  placar 4x0,  epsilon: 0.010\n",
      "epis√≥dio 220,  retorno  4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  3450.0,  placar 4x0,  epsilon: 0.010\n",
      "epis√≥dio 221,  retorno  3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  3400.0,  placar 4x1,  epsilon: 0.010\n",
      "epis√≥dio 222,  retorno  4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  3400.0,  placar 4x0,  epsilon: 0.010\n",
      "epis√≥dio 223,  retorno  3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  3350.0,  placar 4x1,  epsilon: 0.010\n",
      "epis√≥dio 224,  retorno  3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  3300.0,  placar 4x1,  epsilon: 0.010\n",
      "epis√≥dio 225,  retorno  4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  3300.0,  placar 4x0,  epsilon: 0.010\n",
      "epis√≥dio 226,  retorno  3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  3350.0,  placar 4x1,  epsilon: 0.010\n",
      "epis√≥dio 227,  retorno  4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  3750.0,  placar 4x0,  epsilon: 0.010\n",
      "epis√≥dio 228,  retorno  3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  3750.0,  placar 4x1,  epsilon: 0.010\n",
      "epis√≥dio 229,  retorno  4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  3750.0,  placar 4x0,  epsilon: 0.010\n",
      "epis√≥dio 230,  retorno  3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  3700.0,  placar 4x1,  epsilon: 0.010\n",
      "epis√≥dio 231,  retorno  3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  3700.0,  placar 4x1,  epsilon: 0.010\n",
      "epis√≥dio 232,  retorno  3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  3650.0,  placar 4x1,  epsilon: 0.010\n",
      "epis√≥dio 233,  retorno  4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  3700.0,  placar 4x0,  epsilon: 0.010\n",
      "epis√≥dio 234,  retorno  4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  3750.0,  placar 4x0,  epsilon: 0.010\n",
      "epis√≥dio 235,  retorno  3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  3700.0,  placar 4x1,  epsilon: 0.010\n",
      "epis√≥dio 236,  retorno  3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  3700.0,  placar 4x1,  epsilon: 0.010\n",
      "epis√≥dio 237,  retorno  4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  3700.0,  placar 4x0,  epsilon: 0.010\n",
      "epis√≥dio 238,  retorno  3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  3700.0,  placar 4x1,  epsilon: 0.010\n",
      "epis√≥dio 239,  retorno  3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  3650.0,  placar 4x1,  epsilon: 0.010\n",
      "epis√≥dio 240,  retorno  3500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  3650.0,  placar 4x1,  epsilon: 0.010\n",
      "epis√≥dio 241,  retorno  -500.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  3250.0,  placar 1x2,  epsilon: 0.010\n",
      "epis√≥dio 242,  retorno  4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  3300.0,  placar 4x0,  epsilon: 0.010\n",
      "epis√≥dio 243,  retorno  4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  3300.0,  placar 4x0,  epsilon: 0.010\n",
      "epis√≥dio 244,  retorno  4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  3300.0,  placar 4x0,  epsilon: 0.010\n",
      "epis√≥dio 245,  retorno  4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  3350.0,  placar 4x0,  epsilon: 0.010\n",
      "epis√≥dio 246,  retorno  4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  3400.0,  placar 4x0,  epsilon: 0.010\n",
      "epis√≥dio 247,  retorno  4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  3400.0,  placar 4x0,  epsilon: 0.010\n",
      "epis√≥dio 248,  retorno  4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  3450.0,  placar 4x0,  epsilon: 0.010\n",
      "epis√≥dio 249,  retorno  3000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  3400.0,  placar 4x2,  epsilon: 0.010\n",
      "epis√≥dio 250,  retorno  4000.0,  retorno m√©dio (√∫ltimos 10 epis√≥dios)  3450.0,  placar 4x0,  epsilon: 0.010\n"
     ]
    }
   ],
   "source": [
    "treina(env, Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèì Testando nosso Agente Treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retorno 3500.0,  placar 4x1\n"
     ]
    }
   ],
   "source": [
    "roda_partida(env, Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
